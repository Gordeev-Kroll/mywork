# -*- coding: utf-8 -*-
import nltk
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import twitter_samples, stopwords
from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize
from nltk import FreqDist, classify, NaiveBayesClassifier
import pickle
import re, string, random

# –£–¥–∞–ª–µ–Ω–∏–µ —à—É–º–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞
def remove_noise(tweet_tokens, stop_words=()):
    cleaned_tokens = []

    for token, tag in pos_tag(tweet_tokens, lang='rus'):
        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\(\),]|' \
                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', token)
        token = re.sub("(@[A-Za-z0-9_]+)", "", token)

        if tag.startswith("NN"):
            pos = 'n'
        elif tag.startswith('VB'):
            pos = 'v'
        else:
            pos = 'a'

        lemmatizer = WordNetLemmatizer()
        token = lemmatizer.lemmatize(token, pos)

        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:
            cleaned_tokens.append(token.lower())
    return cleaned_tokens

# –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Å–ª–æ–≤ –∏–∑ —Å–ø–∏—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤
def get_all_words(cleaned_tokens_list):
    for tokens in cleaned_tokens_list:
        for token in tokens:
            yield token

# –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
def get_tweets_for_model(cleaned_tokens_list):
    for tweet_tokens in cleaned_tokens_list:
        yield dict([token, True] for token in tweet_tokens)

# –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞
def get_tokens_from_text(input_text_path):
    with open(input_text_path, 'r', encoding='utf-8') as input_file:
        text = input_file.read()
        lines = text.split('\n')
        res = [word_tokenize(line) for line in lines]
        return res


if __name__ == "__main__":

    stop_words = stopwords.words('russian')

    pos_tweet_tokens_train = get_tokens_from_text('pos_prepared_rusentitweet_train.txt')
    pos_tweet_tokens_test = get_tokens_from_text('pos_prepared_rusentitweet_test.txt')
    positive_tweet_tokens = pos_tweet_tokens_train + pos_tweet_tokens_test

    neg_tweet_tokens_train = get_tokens_from_text('neg_prepared_rusentitweet_train.txt')
    neg_tweet_tokens_test = get_tokens_from_text('neg_prepared_rusentitweet_test.txt')
    negative_tweet_tokens = neg_tweet_tokens_train + neg_tweet_tokens_test

    positive_cleaned_tokens_list = []
    negative_cleaned_tokens_list = []

    # –ò–∑–≤–ª–µ–∫–∞—é—Ç—Å—è —Ç–æ–∫–µ–Ω—ã –∏–∑ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
    for tokens in positive_tweet_tokens:
        positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))

    # –ò–∑–≤–ª–µ–∫–∞—é—Ç—Å—è —Ç–æ–∫–µ–Ω—ã –∏–∑ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
    for tokens in negative_tweet_tokens:
        negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))

    all_pos_words = get_all_words(positive_cleaned_tokens_list)

    # 10 –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏—Ö—Å—è —Å–ª–æ–≤ –∏–∑ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
    freq_dist_pos = FreqDist(all_pos_words)
    print(freq_dist_pos.most_common(10))

    # –°–æ–∑–¥–∞—é—Ç—Å—è –¥–∞—Ç–∞—Å–µ—Ç—ã
    positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)
    negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)

    positive_dataset = [(tweet_dict, "–ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è")
                        for tweet_dict in positive_tokens_for_model]

    negative_dataset = [(tweet_dict, "–ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è")
                        for tweet_dict in negative_tokens_for_model]

    # –î–∞—Ç–∞—Å–µ—Ç—ã –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –≤ –æ–¥–∏–Ω –∏ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞—é—Ç—Å—è

    dataset = positive_dataset + negative_dataset

    random.shuffle(dataset)
    print(len(dataset))
    train_data = dataset[:int(len(dataset) * .85)]
    test_data = dataset[int(len(dataset) * .85):]

    # –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    classifier = NaiveBayesClassifier.train(train_data)

    # –û—Ü–µ–Ω–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    print("Accuracy is:", classify.accuracy(classifier, test_data))

    # –ù–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    print(classifier.show_most_informative_features(10))

    custom_tweets = [
        "–ö–∞–∫ –ø—Ä–µ–∫—Ä–∞—Å–µ–Ω —ç—Ç–æ—Ç –¥–µ–Ω—å! –°–æ–ª–Ω—Ü–µ —Å–≤–µ—Ç–∏—Ç —è—Ä–∫–æ, –ø—Ç–∏—Ü—ã –ø–æ—é—Ç, –∏ –≤–æ–∑–¥—É—Ö –Ω–∞–ø–æ–ª–Ω–µ–Ω –∞—Ä–æ–º–∞—Ç–∞–º–∏ —Ü–≤–µ—Ç–æ–≤. –Ø –ø–æ–ª–æ–Ω —ç–Ω–µ—Ä–≥–∏–∏ –∏ –≥–æ—Ç–æ–≤ —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ—Ç –¥–µ–Ω—å –Ω–µ–∑–∞–±—ã–≤–∞–µ–º—ã–º!",
        "—Å–ø–∞—Å–∏–±–æ –±–æ–ª—å—à–æ–µ –∑–∞ –ø–µ—Ä–µ–≤–æ–¥ —Ç—ã –±–æ–ª—å—à–∞—è —É–º–Ω–∏—á–∫–∞üß°",
        "—è –∫—É–ø–∏–ª–∞ —Å–∞–º—ã–π –º–∏–ª—ã–π —á–µ—Ö–æ–ª –Ω–∞ –ø–ª–∞–Ω–µ—Ç–µ –≤—Å–µ–≥–æ –∑–∞ 149 —Ä—É–±–ª–µ–π",
        "–ë–æ–ª—å—à–µ 600 —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –∏ —Å—Ç–∞—Ä—à–µ–∫–ª–∞—Å—Å–Ω–∏–∫–æ–≤ –∏–∑ –í–æ–ª–≥–æ–≥—Ä–∞–¥–∞, –í–æ–ª–∂—Å–∫–æ–≥–æ, –ö–∞–º—ã—à–∏–Ω–∞, –£—Ä—é–ø–∏–Ω—Å–∫–∞, –ú–∏—Ö–∞–π–ª–æ–≤–∫–∏, –§—Ä–æ–ª–æ–≤–æ –∏ –¥—Ä—É–≥–∏—Ö —Ä–∞–π–æ–Ω–æ–≤ –æ–±–ª–∞—Å—Ç–∏ –ø—Ä–∏–º—É—Ç —É—á–∞—Å—Ç–∏–µ –≤ –î–Ω–µ —Ä–µ–≥–∏–æ–Ω–∞ –Ω–∞ –≤—ã—Å—Ç–∞–≤–∫–µ ¬´–†–æ—Å—Å–∏—è¬ª –≤ –ú–æ—Å–∫–≤–µ. –û—Ç–ª–∏—á–Ω–∏–∫–∏ —É—á–µ–±—ã –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ü–µ–Ω–µ –í–î–ù–• –∏—Å–ø–æ–ª–Ω—è—Ç –º—É–∑—ã–∫–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç–ø–æ—Ö. –í –∏—Ö —á–∏—Å–ª–µ ¬´–ú–∞—Ä—à —ç–Ω—Ç—É–∑–∏–∞—Å—Ç–æ–≤¬ª, ¬´–ú–∞—Ä—à –†–∞–¥–µ—Ü–∫–æ–≥–æ¬ª –∏ ¬´–ü—Ä–æ—â–∞–Ω–∏–µ —Å–ª–∞–≤—è–Ω–∫–∏¬ª. –í—Å–µ–≥–æ –∂–µ –≤ —ç—Ç–æ—Ç –¥–µ–Ω—å –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ –æ–∫–æ–ª–æ 50 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –Ω–∞ 10 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–ª–æ—â–∞–¥–∫–∞—Ö. –ì—É–±–µ—Ä–Ω–∞—Ç–æ—Ä –í–æ–ª–≥–æ–≥—Ä–∞–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏ –ê–Ω–¥—Ä–µ–π –ë–æ—á–∞—Ä–æ–≤ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ä–µ–≥–∏–æ–Ω–∞ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ñ–µ—Ä–∞—Ö –∂–∏–∑–Ω–∏, –≤–∫–ª—é—á–∞—è –∏—Å–∫—É—Å—Å—Ç–≤–æ, —Ç—É—Ä–∏–∑–º, –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ü–∏–∞–ª—å–Ω—É—é —Å—Ñ–µ—Ä—É. ¬´–¢–∞–∫–∂–µ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω—ã –ø–æ–¥–ø–∏—Å–∞–Ω–∏—è —Å–æ–≥–ª–∞—à–µ–Ω–∏–π; –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤; –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –æ–±—Ä—è–¥–∞ –¥–æ–Ω—Å–∫–æ–π –∫–∞–∑–∞—á—å–µ–π —Å–≤–∞–¥—å–±—ã —Å –≤—ã–∫—É–ø–æ–º, –∫–∞—Ä–∞–≤–∞–µ–º –∏ —Ç–∞–Ω—Ü–∞–º–∏; –º–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å—ã; –≥–∞–ª–∞-–∫–æ–Ω—Ü–µ—Ä—Ç —Å —É—á–∞—Å—Ç–∏–µ–º –∑–≤—ë–∑–¥ —Ä–µ–≥–∏–æ–Ω–∞; —Ä–∞–±–æ—Ç–∞ –≥–∞—Å—Ç—Ä–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π –ø–ª–æ—â–∞–¥–∫–∏ ‚Äî –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π –≤—ã—Å—Ç–∞–≤–∫–∏ —É–≥–æ—Å—Ç—è—Ç –±–ª—é–¥–∞–º–∏ —Å –±–µ—Ä–µ–≥–æ–≤ –í–æ–ª–≥–∏ –∏ –î–æ–Ω–∞¬ª, ‚Äî  –ø–æ–¥–µ–ª–∏–ª–∏—Å—å –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç—è–º–∏ –≤ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–∏ –í–æ–ª–≥–æ–≥—Ä–∞–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏.",
        '–ì–ª–∞–≤–∞ –í–æ–ª–≥–æ–≥—Ä–∞–¥–∞ –≤–æ–∑–ª–æ–∂–∏–ª —Ü–≤–µ—Ç—ã –∫ –º–µ–º–æ—Ä–∏–∞–ª—å–Ω–æ–π –¥–æ—Å–∫–µ. 29 –¥–µ–∫–∞–±—Ä—è –í–æ–ª–≥–æ–≥—Ä–∞–¥ –∏ –≤—Å—è —Å—Ç—Ä–∞–Ω–∞ –≤—Å–ø–æ–º–∏–Ω–∞—é—Ç —Ç—Ä–∞–≥–∏—á–µ—Å–∫–∏–µ —Å–æ–±—ã—Ç–∏—è 2013 –≥–æ–¥–∞. –í —ç—Ç–æ—Ç –¥–µ–Ω—å 10 –ª–µ—Ç –Ω–∞–∑–∞–¥ –≤ –∑–¥–∞–Ω–∏–∏ –∂–µ–ª–µ–∑–Ω–æ–¥–æ—Ä–æ–∂–Ω–æ–≥–æ –≤–æ–∫–∑–∞–ª–∞ –í–æ–ª–≥–æ–≥—Ä–∞–¥–∞ –±—ã–ª —Å–æ–≤–µ—Ä—à–µ–Ω —Ç–µ—Ä–∞–∫—Ç. –í–∑—Ä—ã–≤ –ø—Ä–æ–∏–∑–æ—à–µ–ª –≤ 12:45. –û–Ω –ø—Ä–æ–≥—Ä–µ–º–µ–ª –Ω–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–∂–µ –≤–æ–∫–∑–∞–ª–∞ –º–µ–∂–¥—É –≤—Ö–æ–¥–æ–º –≤ –∑–¥–∞–Ω–∏–µ –∏ —Ç—É—Ä–Ω–∏–∫–µ—Ç–æ–º. –¢–µ—Ä–∞–∫—Ç —É–Ω–µ—Å –∂–∏–∑–Ω–∏ 18 —á–µ–ª–æ–≤–µ–∫, –¥–µ—Å—è—Ç–∫–∏ –ª—é–¥–µ–π –ø–æ–ª—É—á–∏–ª–∏ —Ä–∞–Ω–µ–Ω–∏—è. –°–µ–≥–æ–¥–Ω—è, 29 –¥–µ–∫–∞–±—Ä—è, –≥–ª–∞–≤–∞ –í–æ–ª–≥–æ–≥—Ä–∞–¥–∞ –í–ª–∞–¥–∏–º–∏—Ä –ú–∞—Ä—á–µ–Ω–∫–æ –≤–æ–∑–ª–æ–∂–∏–ª —Ü–≤–µ—Ç—ã –∫ –º–µ–º–æ—Ä–∏–∞–ª—å–Ω–æ–π –¥–æ—Å–∫–µ, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–π —É –∂–µ–ª–µ–∑–Ω–æ–¥–æ—Ä–æ–∂–Ω–æ–≥–æ –≤–æ–∫–∑–∞–ª–∞ –í–æ–ª–≥–æ–≥—Ä–∞–¥-I. –û–Ω –ø–æ—á—Ç–∏–ª –º–∏–Ω—É—Ç–æ–π –º–æ–ª—á–∞–Ω–∏—è –ø–∞–º—è—Ç—å –∂–µ—Ä—Ç–≤ —Ç–µ—Ä–∞–∫—Ç–∞. –¢–∞–∫–∂–µ —Å–µ–≥–æ–¥–Ω—è –≥–ª–∞–≤–∞ —Ä–µ–≥–∏–æ–Ω–∞ –ê–Ω–¥—Ä–µ–π –ë–æ—á–∞—Ä–æ–≤ –ø–æ—á—Ç–∏–ª –ø–∞–º—è—Ç—å –ø–æ–≥–∏–±—à–∏—Ö –≤ —Ç–µ—Ä–∞–∫—Ç–∞—Ö 2013 –≥–æ–¥–∞.',
        '–û–Ω–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –Ω–∞ —Å—Ç–∞–Ω—Ü–∏–∏ "–ö–æ–º—Å–æ–º–æ–ª—å—Å–∫–∞—è" –∏ –≤ —Å–∫–æ—Ä–æ–º –≤—Ä–µ–º–µ–Ω–∏ –±—É–¥—É—Ç –∑–∞–ø—É—â–µ–Ω—ã –≤ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—é. –°–µ–≥–æ–¥–Ω—è, 3 —è–Ω–≤–∞—Ä—è, –≤ —Ä–∞–º–∫–∞—Ö –≤—ã–µ–∑–¥–Ω–æ–≥–æ —Ä–∞–±–æ—á–µ–≥–æ —Å–æ–≤–µ—â–∞–Ω–∏—è –Ω–∞ —Å—Ç–∞–Ω—Ü–∏–∏ –º–µ—Ç—Ä–æ—Ç—Ä–∞–º–∞ "–ö–æ–º—Å–æ–º–æ–ª—å—Å–∫–∞—è" –≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä –í–æ–ª–≥–æ–≥—Ä–∞–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏ –ê–Ω–¥—Ä–µ–π –ë–æ—á–∞—Ä–æ–≤ –ª–∏—á–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª –Ω–æ–≤—ã–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —ç—Å–∫–∞–ª–∞—Ç–æ—Ä—ã. –í —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—é –æ–Ω–∏ –≤–æ–π–¥—É—Ç –≤ —Å–∞–º–æ–µ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è, –ø–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—ã. –ù–æ–≤—ã–µ —ç—Å–∫–∞–ª–∞—Ç–æ—Ä—ã —Å–∫–æ—Ä–æ –∑–∞—Ä–∞–±–æ—Ç–∞—é—Ç –∏ –Ω–∞ –¥—Ä—É–≥–æ–π –ø–æ–¥–∑–µ–º–Ω–æ–π —Å—Ç–∞–Ω—Ü–∏–∏ - "–ü–ª–æ—â–∞–¥—å –õ–µ–Ω–∏–Ω–∞". –û–±—Å—É–∂–¥–∞—è –ø—Ä–æ—Ü–µ—Å—Å –º–æ–¥–µ—Ä–Ω–∏–∑–∞—Ü–∏–∏ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å–∞–º–æ–≥–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ–≥–æ –≥–æ—Ä–æ–¥—Å–∫–æ–≥–æ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞ –≤ –æ–±–ª–∞—Å—Ç–Ω–æ–π —Å—Ç–æ–ª–∏—Ü–µ, –≥–ª–∞–≤–∞ —Ä–µ–≥–∏–æ–Ω–∞ —Ç–∞–∫–∂–µ –ø–æ—Å—Ç–∞–≤–∏–ª –∑–∞–¥–∞—á—É –ø–æ –≤–Ω–µ–¥—Ä–µ–Ω–∏—é —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –≤ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é —Ä–∞–±–æ—Ç—ã —Å—Ç–∞–Ω—Ü–∏–π, –≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏ —Ç—É—Ä–Ω–∏–∫–µ—Ç–æ–≤. - –ú—ã –≤—ã—Ö–æ–¥–∏–º –Ω–∞ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—é –Ω–æ–≤—ã—Ö —ç—Å–∫–∞–ª–∞—Ç–æ—Ä–æ–≤, –Ω–æ –Ω—É–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∏ –¥–∞–ª—å—à–µ, - –∞–∫—Ü–µ–Ω—Ç–∏—Ä–æ–≤–∞–ª –≤–Ω–∏–º–∞–Ω–∏–µ –ê–Ω–¥—Ä–µ–π –ë–æ—á–∞—Ä–æ–≤, - –≤ —Ö–æ–ª–ª–∞—Ö –ø–æ–¥–∑–µ–º–Ω—ã—Ö —Å—Ç–∞–Ω—Ü–∏–π –±–æ–ª—å—à–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ, –≤ –Ω–∏—Ö –º–æ–∂–Ω–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å —Ç—É—Ä–Ω–∏–∫–µ—Ç—ã. –í —Ü–µ–ª—è—Ö –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –Ω–µ–ª—å–∑—è –æ—Å—Ç–∞–≤–ª—è—Ç—å –∏ –ª–∞–≤–æ—á–∫–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ–º –ø–æ–¥ –Ω–∏–º–∏. –ì—É–±–µ—Ä–Ω–∞—Ç–æ—Ä –æ—Ç–º–µ—Ç–∏–ª, —á—Ç–æ –ø–µ—Ä–≤—ã–π —ç—Ç–∞–ø —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å–∫–æ—Ä–æ—Å—Ç–Ω–æ–≥–æ —Ç—Ä–∞–º–≤–∞—è –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≤–µ—Ä—à–µ–Ω. –î–æ –∫–æ–Ω—Ü–∞ –º–µ—Å—è—Ü–∞ –±—É–¥—É—Ç –∑–∞–≤–µ—Ä—à–µ–Ω—ã –ø—É—Å–∫–æ–Ω–∞–ª–∞–¥–æ—á–Ω—ã–µ —Ä–∞–±–æ—Ç—ã –∏ –Ω–∞—á–Ω—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —ç—Å–∫–∞–ª–∞—Ç–æ—Ä—ã. - –í—Å–µ –∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Å—Ç–∞–≤–∏–ª–∏ –Ω–∞ 2023 –≥–æ–¥, –í–æ–ª–≥–æ–≥—Ä–∞–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å –≤—ã–ø–æ–ª–Ω–∏–ª–∞ –≤ –ø–æ–ª–Ω–æ–º –æ–±—ä–µ–º–µ, - —Å–∫–∞–∑–∞–ª –≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä. - –≠—Ç–æ –∫–∞—Å–∞–µ—Ç—Å—è –∏ –∑–∞–º–µ–Ω—ã —à–ø–∞–ª, —Ä–µ–ª—å—Å–æ–≤, –≤—Å–µ—Ö —Ç–µ—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –≤ —Ç—Ä–µ—Ö—Å—Ç–æ—Ä–æ–Ω–Ω–µ–º –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω–æ–º —Å–æ–≥–ª–∞—à–µ–Ω–∏–∏. –ú—ã –æ—Å—É—â–µ—Å—Ç–≤–∏–ª–∏ –∑–∞–∫—É–ø–∫—É –Ω–æ–≤–æ–≥–æ –ø–æ–¥–≤–∏–∂–Ω–æ–≥–æ —Å–æ—Å—Ç–∞–≤–∞. –í —Ü–µ–ª–æ–º –∑–∞–¥–∞—á–∏ –Ω–∞ 2024 –≥–æ–¥ –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã, –æ–±—ä–µ–º —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤ –ø–æ–ª–Ω–æ—Å—Ç—å—é –µ—Å—Ç—å. –í —Ç–µ–∫—É—â–µ–º –≥–æ–¥—É –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø—É—Ç–µ–π —Å–∫–æ—Ä–æ—Å—Ç–Ω–æ–≥–æ —Ç—Ä–∞–º–≤–∞—è –æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ ¬´–ü–ª. –í–æ–∑—Ä–æ–∂–¥–µ–Ω–∏—è¬ª –¥–æ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ ¬´–¢–†–ö –ï–≤—Ä–æ–ø–∞¬ª. –¢–∞–∫–∂–µ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω—ã –º–æ–¥–µ—Ä–Ω–∏–∑–∞—Ü–∏–∏ —Ç—è–≥–æ–≤—ã—Ö –ø–æ–¥—Å—Ç–∞–Ω—Ü–∏–π, –∫–∞–±–µ–ª—å–Ω—ã—Ö –∏ –≤–æ–∑–¥—É—à–Ω—ã—Ö –ª–∏–Ω–∏–π –°–¢, –≤—ã—Ö–æ–¥ –Ω–∞ –ª–∏–Ω–∏—é –∑–∞–∫—É–ø–ª–µ–Ω–Ω—ã—Ö 50 –æ–¥–Ω–æ—Å–µ–∫—Ü–∏–æ–Ω–Ω—ã—Ö –∏ 12 —Ç—Ä–µ—Ö—Å–µ–∫—Ü–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–º–≤–∞–µ–≤;, –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —É—á–∞—Å—Ç–∫–æ–≤ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –∏ –º–æ–¥–µ—Ä–Ω–∏–∑–∞—Ü–∏–∏ (–Ω–æ–≤–æ–≥–æ —É—á–∞—Å—Ç–∫–∞ –°–¢ –¥–æ –¢–†–ö ¬´–ê–∫–≤–∞—Ä–µ–ª—å¬ª), —Ç—Ä–∞–º–≤–∞–π–Ω—ã—Ö –ª–∏–Ω–∏–π –≤ –ö—Ä–∞—Å–Ω–æ–∞—Ä–º–µ–π—Å–∫–æ–º —Ä–∞–π–æ–Ω–µ.'
    ]
    sentiments = [
        "pos",  # "Just had an amazing experience with AwesomeServices, their customer support is top-notch!"
        "pos",  # "I can't believe the quality of the product I received from FantasticGoods, exceeded my expectations!"
        "pos",  # "Ordered from SuperQuickDelivery and got my package within hours. Impressed!"
        "neg",  # "Had a terrible experience with SlowServiceInc, will never use their services again."
        "neg",  # "In love with the new features of the latest app update. Great job, TechInnovators!"
        "neg",  # "Huge shoutout to DeliciousEats for the tasty food. Will definitely order again!"
    ]
    for i, tweet in enumerate(custom_tweets):
        custom_tokens = remove_noise(word_tokenize(tweet))
        print(f"{i + 1})\n\t{tweet}")
        print("\tPredict: ", classifier.classify(dict([token, True] for token in custom_tokens)))
        print(f"\tReal: {sentiments[i]}\n")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    f = open('my_classifier.pickle', 'wb')
    pickle.dump(classifier, f)
    f.close()

# nltk.download('wordnet')
# nltk.download('averaged_perceptron_tagger_ru')
# nltk.download('stopwords')
#
